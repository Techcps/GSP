{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Intro to Context Caching with the Gemini API\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fcontext-caching%2Fintro_context_caching.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases.\n",
    "\n",
    "### Context Caching\n",
    "\n",
    "The Gemini API provides the context caching feature for developers to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model. This feature can help reduce the number of tokens sent to the model, thereby lowering the cost of requests that contain repeat content with high input token counts.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "In this tutorial, you learn how to use the Gemini API context caching feature in Vertex AI.\n",
    "\n",
    "You will complete the following tasks:\n",
    "- Create a context cache\n",
    "- Retrieve and use a context cache\n",
    "- Use context caching in Chat\n",
    "- Update the expire time of a context cache\n",
    "- Delete a context cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XRvKdaPDTznN",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-02-829ab59dc5b4\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-west1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHwJCyNF6u0O"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mginH0QC6u0O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import Part\n",
    "from vertexai.preview import caching\n",
    "from vertexai.preview.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAGPrpYagu2z"
   },
   "source": [
    "### Create a context cache\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-1.5-pro-001`). You must include the version postfix (for example, the `-001` in `gemini-1.5-pro-001`).\n",
    "\n",
    "For more information, see [Available Gemini stable model versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versioning#stable-versions-available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XdpfXqpJ-NNj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-1.5-pro-001\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "appJ7LCc_YCW"
   },
   "source": [
    "Context caching is particularly well suited to scenarios where a substantial initial context is referenced repeatedly by shorter requests.\n",
    "\n",
    "- Cached content can be any of the MIME types supported by Gemini multimodal models. For example, you can cache a large amount of text, audio, or video. **Note**: The minimum size of a context cache is 32,769 tokens.\n",
    "- The default expiration time of a context cache is 60 minutes. You can specify a different expiration time using the `ttl` (time to live) or the `expire_time` property.\n",
    "\n",
    "This example shows how to create a context cache using two large research papers stored in a Cloud Storage bucket, and set the `ttl` to 60 minutes.\n",
    "\n",
    "- Paper 1: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)\n",
    "- Paper 2: [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UmJA6AvVujyZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = caching.CachedContent.create(\n",
    "    model_name=MODEL_ID,\n",
    "    system_instruction=system_instruction,\n",
    "    contents=contents,\n",
    "    ttl=datetime.timedelta(minutes=60),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e1dKGSLDg2q"
   },
   "source": [
    "You can access the properties of the cached content as example below. You can use its `name` or `resource_name` to reference the contents of the context cache.\n",
    "\n",
    "**Note**: The `name` of the context cache is also referred to as cache ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uRJPRtkKDk2b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309079971445866496\n",
      "projects/379201932255/locations/us-west1/cachedContents/2309079971445866496\n",
      "projects/qwiklabs-gcp-02-829ab59dc5b4/locations/us-west1/publishers/google/models/gemini-1.5-pro-001\n",
      "2024-10-19 01:29:47.999229+00:00\n",
      "2024-10-19 02:29:47.800884+00:00\n"
     ]
    }
   ],
   "source": [
    "print(cached_content.name)\n",
    "print(cached_content.resource_name)\n",
    "print(cached_content.model_name)\n",
    "print(cached_content.create_time)\n",
    "print(cached_content.expire_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-f5gTEaCPkN"
   },
   "source": [
    "### Retrieve and use a context cache\n",
    "\n",
    "You can use the property `name` or `resource_name` to reference the contents of the context cache. For example:\n",
    "```\n",
    "new_cached_content = caching.CachedContent(cached_content_name=cached_content.name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ1zMmFQ1BNj"
   },
   "source": [
    "To use the context cache, you construct a `GenerativeModel` with the context cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EPVyJIW1BaVj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel.from_cached_content(cached_content=cached_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kgfyCoGH_w0"
   },
   "source": [
    "Then you can query the model with a prompt, and the cached content will be used as a prefix to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5dSDogewDAHB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research papers share the goal of developing highly capable multimodal AI models, particularly focusing on enhancing long-context understanding capabilities without compromising performance on other tasks. This is achieved through innovations in model architecture (mixture-of-experts), training and serving infrastructure, and data curation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"What is the research goal shared by these research papers?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX7vHiybEWeJ"
   },
   "source": [
    "### Use context caching in Chat\n",
    "\n",
    "print(Please like share & subscribe to Techcps https://www.youtube.com/@techcps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX7vHiybEWeJ"
   },
   "source": [
    "### Use context caching in Chat\n",
    "\n",
    "You can use the context cache in a multi-turn chat session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FYNZS5o0FoGR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5U-6wGSFFx51",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini 1.5's responsible AI development focuses on mitigating the potential negative impacts of longer context lengths on safety and fairness, while Gemini 1.0 focused primarily on text generation, understanding, and image and video understanding. Specifically, Gemini 1.5 incorporates new image-to-text safety training data and more challenging evaluations that consider the interplay between safety violations and demographic groups. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How do the approaches to responsible AI development and mitigation strategies in Gemini 1.5 evolve from those in Gemini 1.0?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FFO_JgKNeCpK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The papers highlight the need for more challenging and robust evaluations, particularly for long-context and multimodal tasks, to better understand LLM capabilities as they become increasingly sophisticated. Additionally, further research is needed on mitigating \"hallucinations\" and improving high-level reasoning abilities such as causal understanding, logical deduction, and counterfactual reasoning in LLMs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Given the advancements presented in Gemini 1.5, what are the key future research directions identified in both papers\n",
    "for further improving multimodal AI models?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2VhjUmojQjg"
   },
   "source": [
    "You can use `print(chat.history)` to print out the chat session history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORsGDdXXLwHK"
   },
   "source": [
    "### Update the expiration time of a context cache\n",
    "\n",
    "\n",
    "The default expiration time of a context cache is 60 minutes. To update the expiration time, update one of the following properties:\n",
    "\n",
    "`ttl` - The number of seconds and nanoseconds that the cache lives after it's created or after the `ttl` is updated before it expires. When you set the `ttl`, the cache `expire_time` is updated.\n",
    "\n",
    "`expire_time` - A Timestamp that specifies the absolute date and time when the context cache expires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WyiZoZHKI2Jr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-19 02:31:26.540678+00:00\n"
     ]
    }
   ],
   "source": [
    "cached_content.update(ttl=datetime.timedelta(hours=1))\n",
    "\n",
    "cached_content.refresh()\n",
    "\n",
    "print(cached_content.expire_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chd6_8YRxdIu"
   },
   "source": [
    "### Delete a context cache\n",
    "\n",
    "You can remove content from the cache using the delete operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XGzgTk6YzgSt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CachedContent : projects/379201932255/locations/us-west1/cachedContents/2309079971445866496\n"
     ]
    }
   ],
   "source": [
    "cached_content.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_context_caching.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
