{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c84ab5-d70c-48ee-b9ac-cfc308b824ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-google-genai chromadb\n",
    "!pip install langchain-community\n",
    "!pip install langchain-google-vertexai\n",
    "# Please ignore error messages related to pip's dependency resolver issues and proceed furthur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84400dd7-e520-438b-bc51-387e94c0219b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff9cd4-76cd-417d-920a-8dad29cc68c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-02-c8af5f36fea8\"  # @param {type:\"string\"} Please set your PROJECT_ID\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5f25b-0ecc-4d47-b676-1705befc959c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fae3e-703d-4b16-bd82-323bae373d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 1. Use the WebBaseLoader to load documents related to queries\n",
    "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-ai/\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a78e63-2c30-48f6-a4ee-ef78d2645dc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Split text into chunks\n",
    "Now that we have the documents we will split them into chunks. Each chunk will become one vector in the vector store. To do this we will define a chunk size (number of characters) and a chunk overlap (amount of overlap i.e. sliding window). The perfect chunk size can be difficult to determine. Too large of a chunk size leads to too much information per chunk (individual chunks not specific enough), however too small of a chunk size leads to not enough information per chunk. In both cases, nearest neighbors lookup with a query/question embedding may struggle to retrieve the actually relevant chunks, or fail altogether if the chunks are too large to use as context with an LLM query.\n",
    "\n",
    "In this notebook we will use a chunk size of 800 chacters and a chunk overlap of 400 characters, but feel free to experiment with other sizes! Note: you can specify a custom `length_function` with `RecursiveCharacterTextSplitter` if you want chunk size/overlap to be determined by something other than Python's `len` function. In addition to `RecursiveCharacterTextSplitter`, there are [other text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token) you can consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f7fa7-06b0-4018-a714-ac89e05adfbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 2. Use the RecursiveCharacterTextSplitter class to split the documents into chunks for embedding\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Look at the first two chunks \n",
    "chunks[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479a543-2382-4359-a15f-96d3e86b732e",
   "metadata": {},
   "source": [
    "### Vectorize/Embed Document Chunks\n",
    "Now we need to embed the document chunks (turn them into vectors) and store them in a vector store. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the PaLM API for Embeddings. The LangChain library provides a wrapper class around the PaLM Embeddings API, `VertexAIEmbeddings()`.\n",
    "\n",
    "For the purposes of this lab, you will use [Chroma](https://www.trychroma.com/) as the vector store for simplicity. In a real-world scenario with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f212c-461e-468a-a2c2-4461a1f4cf55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 3. Create vector store using embeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd86e73-ccf5-4cdb-938d-899ebae27b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc41f30-c0d8-4365-b844-a735e5e4c763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,                   # Data\n",
    "    embedding=gemini_embeddings,      # Embedding model\n",
    "    persist_directory=\"./vectorstore\" # Directory to save data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef299c9-cad7-440f-91f8-9f1f7a065b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "    persist_directory=\"./vectorstore\",     # Directory of db\n",
    "    embedding_function=gemini_embeddings   # Embedding model\n",
    ")\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(len(retriever.invoke(\"MMLU\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcec8b4-575d-47e4-b0b6-1fa1378fc46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Putting it all together\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and PaLM as our LLM. Langchain provides a wrapper around PaLM, `VertexAI()`. \n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255dce30-5f7e-49dc-a4f2-635120c85775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store \n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":2} # number of nearest neighbors to retrieve  \n",
    ")\n",
    "\n",
    "# You can also set temperature, top_p, top_k \n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison\",\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "# q/a chain \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b841b15-26fb-4dd1-bc75-16901412f02e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query \n",
    "Now that everything is \"chained\" together using LangChain we can send queries and get answers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50eb258-abe5-4628-be1d-0b63775d0a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    print(f\"Response: {response['result']}\\n\")\n",
    "\n",
    "    citations = {doc.metadata['source'] for doc in response['source_documents']}\n",
    "    print(f\"Citations: {citations}\\n\")\n",
    "\n",
    "    # uncomment below to print source chunks used  \n",
    "    # print(f\"Source Chunks Used: {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113eb28a-5af6-4a5c-96f4-2d7d6f5a68c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"What is MMLU?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b400e66-fc81-4f56-83a9-deac64a49a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"What is a TPU?\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
